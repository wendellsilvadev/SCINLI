# SCINLI - Infer√™ncia de Linguagem Natural Cient√≠fica com RoBERTa

Este projeto implementa um modelo de **Infer√™ncia de Linguagem Natural (NLI)** voltado para o dom√≠nio **cient√≠fico**, utilizando o dataset [SCINLI](https://huggingface.co/datasets/tasksource/scinli) e o modelo pr√©-treinado **RoBERTa-base** da biblioteca Transformers.  
Ele √© inspirado no artigo **"Co-Training for Low Resource Scientific Natural Language Inference" (ACL 2024)**, que prop√µe o uso de *Weighted Co-Training (WCT)* para melhorar o aprendizado em cen√°rios com poucos dados rotulados.

---

##  Objetivo do Projeto

Treinar um modelo capaz de **entender a rela√ß√£o entre duas senten√ßas cient√≠ficas**, determinando se:
- Uma **implica** a outra (*entailment*);
- Elas se **contradizem** (*contrasting*);
- S√£o **neutras** (sem rela√ß√£o direta);
- Ou envolvem **racioc√≠nio cient√≠fico** (*reasoning*).

---

## Instala√ß√£o e Execu√ß√£o

### Clonar o reposit√≥rio
```bash
git clone https://github.com/SEU_USUARIO/scinli-wct.git
cd scinli-wct

```

## Criar ambiente virtual

```bash
python -m venv venv
venv\Scripts\activate    # Windows
source venv/bin/activate # Linux/macOS
```

## Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

## Executar o treinamento

```bash
cd scinli-wct
python train_wct.py
```


# O modelo ser√° treinado e voc√™ ver√° sa√≠das como:
```bash
Carregando dataset SCINLI...
Selecionando pequeno subconjunto rotulado (Dl)...
Conjunto Dl criado com 400 amostras.
Tokenizando textos...
Treinando modelo base...
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:18<00:00, 1.56s/it]
Loss m√©dio: 1.3981
Treinamento inicial conclu√≠do!
```

## Funcionamento do C√≥digo
Entrada

O modelo recebe duas senten√ßas cient√≠ficas:

    sentence1: a premissa

    sentence2: a hip√≥tese

Exemplo:

sentence1 = "Increasing temperature raises reaction rate."
sentence2 = "Reactions are faster at higher temperatures."

## Processamento

O modelo RoBERTa-base analisa as duas senten√ßas simultaneamente e gera representa√ß√µes vetoriais de contexto, entendendo o significado das palavras e as rela√ß√µes entre as senten√ßas.
## Classifica√ß√£o

A camada final (classifier) transforma a representa√ß√£o em 4 probabilidades, uma para cada rela√ß√£o:
Classe	Significado	Exemplo
0	Contrasting	‚ÄúO tratamento reduziu a dor.‚Äù / ‚ÄúA dor aumentou.‚Äù
1	Reasoning	‚ÄúA press√£o aumentou.‚Äù / ‚ÄúA temperatura pode ter subido.‚Äù
2	Entailment	‚ÄúO sol aquece a Terra.‚Äù / ‚ÄúA Terra recebe calor do sol.‚Äù
3	Neutral	‚ÄúA amostra foi aquecida.‚Äù / ‚ÄúA mistura foi resfriada.‚Äù

A classe com maior probabilidade √© a previs√£o final do modelo.
## Aprendizado

Durante o treino:

O modelo faz previs√µes;

Calcula o erro (loss) comparando com o r√≥tulo real;

O otimizador AdamW ajusta os pesos internos;

O processo se repete por v√°rias √©pocas, reduzindo o loss e melhorando a precis√£o.

Arquivo config.yaml

```bash

model_name: "roberta-base"
batch_size: 8
lr: 2e-5
epochs_init: 1
epochs_cotraining: 1
epochs_finetune: 1
max_length: 128
`
Como ajustar para um treinamento mais completo

O arquivo config.yaml controla todo o comportamento do treinamento.  
Se o usu√°rio quiser um treinamento mais longo, mais preciso ou com mais dados, basta modificar alguns par√¢metros ali:
```
---

### Par√¢metros principais

| Par√¢metro | Fun√ß√£o | Valor padr√£o | Como ajustar para mais treino |
|------------|---------|---------------|-------------------------------|
| `epochs_init` | Quantas vezes o modelo v√™ o subset rotulado `Dl` | `1` | Aumente para `3`‚Äì`10` para treinar mais |
| `per_class_small_Dl` | Quantas amostras por classe usar no subset rotulado | `100` | Aumente para `300`‚Äì`1000` para mais dados |
| `batch_size` | Quantas amostras por itera√ß√£o | `8` | Pode subir para `16` ou `32` (se tiver GPU) |
| `lr` | Taxa de aprendizado | `2e-5` | Pode reduzir (ex: `1e-5`) se aumentar as √©pocas |
| `epochs_finetune` | √âpocas adicionais de fine-tuning com todo o dataset | `1` | Aumente para `3`‚Äì`5` para melhorar desempenho |
| `device` | CPU ou GPU | `"cuda"` | Troque para `"cpu"` se n√£o tiver GPU NVIDIA |

---

### Recomenda√ß√µes pr√°ticas

- **Quer mais qualidade:** aumente `epochs_init` e `per_class_small_Dl`.  
- **Quer treinar mais r√°pido:** reduza `per_class_small_Dl` e use `batch_size` pequeno.  
- **Quer o m√°ximo poss√≠vel:** use `epochs_init: 5`, `epochs_finetune: 3`, e `per_class_small_Dl: 500`.  
- **Sem GPU:** mude `device: "cpu"` ‚Äî o c√≥digo funciona igual, apenas mais lento.

---

###  Exemplo de configura√ß√£o ‚Äúforte‚Äù

```yaml
model_name: "roberta-base"
batch_size: 16
lr: 1e-5
epochs_init: 5
epochs_cotraining: 1
epochs_finetune: 3
max_length: 128
seed: 42
device: "cuda"
per_class_small_Dl: 500
seed: 42
device: "cuda"     # ou "cpu"
per_class_small_Dl: 100
```

---

## üìä Resultados do Treinamento

O modelo **RoBERTa-base** foi treinado 5 vezes de forma independente, cada execu√ß√£o com **3 √©pocas**, utilizando o dataset cient√≠fico **SCINLI** (Scientific Natural Language Inference).  
Foram avaliadas as m√©tricas de **Acur√°cia (Accuracy)** e **F1-Score Macro** para os conjuntos de **valida√ß√£o** e **teste**.

### üß™ Tabela de Resultados

| Run | Val Acc | Val F1  | Test Acc | Test F1 |
|-----|----------|--------|----------|---------|
| 1 | 0.2333 | 0.1035 | 0.2133 | 0.0882 |
| 2 | 0.3367 | 0.2950 | 0.3667 | 0.3473 |
| 3 | 0.5433 | 0.5034 | 0.5733 | 0.5444 |
| 4 | 0.5567 | 0.5438 | 0.5033 | 0.4798 |
| 5 | 0.5567 | 0.5284 | 0.5433 | 0.5313 |

 **M√©dias finais (5 execu√ß√µes):**
- Val Accuracy m√©dia ‚Üí **0.4453**
- Val F1 m√©dia ‚Üí **0.3948**

---

## Interpreta√ß√£o dos Resultados

Os resultados mostram que o modelo RoBERTa-base foi capaz de aprender rela√ß√µes sem√¢nticas cient√≠ficas entre pares de senten√ßas, alcan√ßando valores m√©dios de aproximadamente 44% de acur√°cia** e 39% de F1 no conjunto de valida√ß√£o.

Esses n√∫meros indicam um aprendizado efetivo, mas ainda limitado pela pequena quantidade de dados rotulados (150 exemplos por classe).  
Mesmo assim, o modelo conseguiu capturar padr√µes lingu√≠sticos relevantes, demonstrando que o SCINLI pode ser utilizado com sucesso em tarefas de infer√™ncia natural no dom√≠nio cient√≠fico.

### Conclus√£o t√©cnica

- O modelo **n√£o apresentou overfitting**, diferentemente de vers√µes anteriores.
- O aumento gradual de desempenho ao longo das execu√ß√µes mostra consist√™ncia no aprendizado.
- Para resultados mais robustos, pode-se aumentar:
  - o n√∫mero de exemplos rotulados (`per_class_small_Dl`);
  - o n√∫mero de √©pocas (`epochs_init`);
  - ou realizar *fine-tuning* com o conjunto completo (`epochs_finetune > 0`).

---

### Arquivo de resultados

Todos os resultados foram automaticamente salvos em **`outputs/resultados.csv`**


