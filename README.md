# SCINLI - Infer√™ncia de Linguagem Natural Cient√≠fica com RoBERTa

Este projeto implementa um modelo de **Infer√™ncia de Linguagem Natural (NLI)** voltado para o dom√≠nio **cient√≠fico**, utilizando o dataset [SCINLI](https://huggingface.co/datasets/tasksource/scinli) e o modelo pr√©-treinado **RoBERTa-base** da biblioteca Transformers.  
Ele √© inspirado no artigo **"Co-Training for Low Resource Scientific Natural Language Inference" (ACL 2024)**, que prop√µe o uso de *Weighted Co-Training (WCT)* para melhorar o aprendizado em cen√°rios com poucos dados rotulados.

---

##  Objetivo do Projeto

Treinar um modelo capaz de **entender a rela√ß√£o entre duas senten√ßas cient√≠ficas**, determinando se:
- Uma **implica** a outra (*entailment*);
- Elas se **contradizem** (*contrasting*);
- S√£o **neutras** (sem rela√ß√£o direta);
- Ou envolvem **racioc√≠nio cient√≠fico** (*reasoning*).

---

## Instala√ß√£o e Execu√ß√£o

### Clonar o reposit√≥rio
```bash
git clone https://github.com/SEU_USUARIO/scinli-wct.git
cd scinli-wct

```

## Criar ambiente virtual

```bash
python -m venv venv
venv\Scripts\activate    # Windows
source venv/bin/activate # Linux/macOS
```

## Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

## Executar o treinamento

```bash
cd scinli-wct
python train_wct.py
```


# O modelo ser√° treinado e voc√™ ver√° sa√≠das como:
```bash
Carregando dataset SCINLI...
Selecionando pequeno subconjunto rotulado (Dl)...
Conjunto Dl criado com 400 amostras.
Tokenizando textos...
Treinando modelo base...
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:18<00:00, 1.56s/it]
Loss m√©dio: 1.3981
‚úÖ Treinamento inicial conclu√≠do!
```

üß© Funcionamento do C√≥digo
Entrada

O modelo recebe duas senten√ßas cient√≠ficas:

    sentence1: a premissa

    sentence2: a hip√≥tese

Exemplo:

sentence1 = "Increasing temperature raises reaction rate."
sentence2 = "Reactions are faster at higher temperatures."

## Processamento

O modelo RoBERTa-base analisa as duas senten√ßas simultaneamente e gera representa√ß√µes vetoriais de contexto, entendendo o significado das palavras e as rela√ß√µes entre as senten√ßas.
## Classifica√ß√£o

A camada final (classifier) transforma a representa√ß√£o em 4 probabilidades, uma para cada rela√ß√£o:
Classe	Significado	Exemplo
0	Contrasting	‚ÄúO tratamento reduziu a dor.‚Äù / ‚ÄúA dor aumentou.‚Äù
1	Reasoning	‚ÄúA press√£o aumentou.‚Äù / ‚ÄúA temperatura pode ter subido.‚Äù
2	Entailment	‚ÄúO sol aquece a Terra.‚Äù / ‚ÄúA Terra recebe calor do sol.‚Äù
3	Neutral	‚ÄúA amostra foi aquecida.‚Äù / ‚ÄúA mistura foi resfriada.‚Äù

A classe com maior probabilidade √© a previs√£o final do modelo.
## Aprendizado

Durante o treino:

O modelo faz previs√µes;

Calcula o erro (loss) comparando com o r√≥tulo real;

O otimizador AdamW ajusta os pesos internos;

O processo se repete por v√°rias √©pocas, reduzindo o loss e melhorando a precis√£o.

Arquivo config.yaml

```bash

model_name: "roberta-base"
batch_size: 8
lr: 2e-5
epochs_init: 1
epochs_cotraining: 1
epochs_finetune: 1
max_length: 128
`
Como ajustar para um treinamento mais completo

O arquivo config.yaml controla todo o comportamento do treinamento.  
Se o usu√°rio quiser um treinamento mais longo, mais preciso ou com mais dados, basta modificar alguns par√¢metros ali:
```
---

### Par√¢metros principais

| Par√¢metro | Fun√ß√£o | Valor padr√£o | Como ajustar para mais treino |
|------------|---------|---------------|-------------------------------|
| `epochs_init` | Quantas vezes o modelo v√™ o subset rotulado `Dl` | `1` | Aumente para `3`‚Äì`10` para treinar mais |
| `per_class_small_Dl` | Quantas amostras por classe usar no subset rotulado | `100` | Aumente para `300`‚Äì`1000` para mais dados |
| `batch_size` | Quantas amostras por itera√ß√£o | `8` | Pode subir para `16` ou `32` (se tiver GPU) |
| `lr` | Taxa de aprendizado | `2e-5` | Pode reduzir (ex: `1e-5`) se aumentar as √©pocas |
| `epochs_finetune` | √âpocas adicionais de fine-tuning com todo o dataset | `1` | Aumente para `3`‚Äì`5` para melhorar desempenho |
| `device` | CPU ou GPU | `"cuda"` | Troque para `"cpu"` se n√£o tiver GPU NVIDIA |

---

### Recomenda√ß√µes pr√°ticas

- **Quer mais qualidade:** aumente `epochs_init` e `per_class_small_Dl`.  
- **Quer treinar mais r√°pido:** reduza `per_class_small_Dl` e use `batch_size` pequeno.  
- **Quer o m√°ximo poss√≠vel:** use `epochs_init: 5`, `epochs_finetune: 3`, e `per_class_small_Dl: 500`.  
- **Sem GPU:** mude `device: "cpu"` ‚Äî o c√≥digo funciona igual, apenas mais lento.

---

###  Exemplo de configura√ß√£o ‚Äúforte‚Äù

```yaml
model_name: "roberta-base"
batch_size: 16
lr: 1e-5
epochs_init: 5
epochs_cotraining: 1
epochs_finetune: 3
max_length: 128
seed: 42
device: "cuda"
per_class_small_Dl: 500
seed: 42
device: "cuda"     # ou "cpu"
per_class_small_Dl: 100
```
