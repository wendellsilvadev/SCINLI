# SCINLI - Infer√™ncia de Linguagem Natural Cient√≠fica com RoBERTa

Este projeto implementa um modelo de **Infer√™ncia de Linguagem Natural (NLI)** voltado para o dom√≠nio **cient√≠fico**, utilizando o dataset [SCINLI](https://huggingface.co/datasets/tasksource/scinli) e o modelo pr√©-treinado **RoBERTa-base** da biblioteca Transformers.  
Ele √© inspirado no artigo **"Co-Training for Low Resource Scientific Natural Language Inference" (ACL 2024)**, que prop√µe o uso de *Weighted Co-Training (WCT)* para melhorar o aprendizado em cen√°rios com poucos dados rotulados.

---

##  Objetivo do Projeto

Treinar um modelo capaz de **entender a rela√ß√£o entre duas senten√ßas cient√≠ficas**, determinando se:
- Uma **implica** a outra (*entailment*);
- Elas se **contradizem** (*contrasting*);
- S√£o **neutras** (sem rela√ß√£o direta);
- Ou envolvem **racioc√≠nio cient√≠fico** (*reasoning*).

---

## Instala√ß√£o e Execu√ß√£o

### Clonar o reposit√≥rio
```bash
git clone https://github.com/SEU_USUARIO/scinli-wct.git
cd scinli-wct

```

## Criar ambiente virtual

```bash
python -m venv venv
venv\Scripts\activate    # Windows
source venv/bin/activate # Linux/macOS
```

## Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

## Executar o treinamento

```bash
python train_wct.py
```


# O modelo ser√° treinado e voc√™ ver√° sa√≠das como:
```bash
Carregando dataset SCINLI...
Selecionando pequeno subconjunto rotulado (Dl)...
Conjunto Dl criado com 400 amostras.
Tokenizando textos...
Treinando modelo base...
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:18<00:00, 1.56s/it]
Loss m√©dio: 1.3981
‚úÖ Treinamento inicial conclu√≠do!
```

üß© Funcionamento do C√≥digo
Entrada

O modelo recebe duas senten√ßas cient√≠ficas:

    sentence1: a premissa

    sentence2: a hip√≥tese

Exemplo:

sentence1 = "Increasing temperature raises reaction rate."
sentence2 = "Reactions are faster at higher temperatures."

## Processamento

O modelo RoBERTa-base analisa as duas senten√ßas simultaneamente e gera representa√ß√µes vetoriais de contexto, entendendo o significado das palavras e as rela√ß√µes entre as senten√ßas.
## Classifica√ß√£o

A camada final (classifier) transforma a representa√ß√£o em 4 probabilidades, uma para cada rela√ß√£o:
Classe	Significado	Exemplo
0	Contrasting	‚ÄúO tratamento reduziu a dor.‚Äù / ‚ÄúA dor aumentou.‚Äù
1	Reasoning	‚ÄúA press√£o aumentou.‚Äù / ‚ÄúA temperatura pode ter subido.‚Äù
2	Entailment	‚ÄúO sol aquece a Terra.‚Äù / ‚ÄúA Terra recebe calor do sol.‚Äù
3	Neutral	‚ÄúA amostra foi aquecida.‚Äù / ‚ÄúA mistura foi resfriada.‚Äù

A classe com maior probabilidade √© a previs√£o final do modelo.
## Aprendizado

Durante o treino:

O modelo faz previs√µes;

Calcula o erro (loss) comparando com o r√≥tulo real;

O otimizador AdamW ajusta os pesos internos;

O processo se repete por v√°rias √©pocas, reduzindo o loss e melhorando a precis√£o.

‚öôÔ∏è Arquivo config.yaml

```bash

model_name: "roberta-base"
batch_size: 8
lr: 2e-5
epochs_init: 1
epochs_cotraining: 1
epochs_finetune: 1
max_length: 128
seed: 42
device: "cuda"     # ou "cpu"
per_class_small_Dl: 100
```
